#!/usr/bin/env python3
"""
Usage example for garak_prompt_injection.py

This script demonstrates how to use the LLM vulnerability scanner
"""

def print_usage_example():
    """Print a comprehensive usage example"""
    
    print("""
üîç LLM vulnerability scanner - Usage Example
===========================================

This script helps you test LLM models for security vulnerabilities using NVIDIA's garak tool.

üìã Prerequisites:
================
1. Miniconda or Anaconda installed
2. Ollama installed and running
3. Either a .gguf model file or an existing Ollama model

üîß Setup Instructions:
====================

1. Install Miniconda (if not already installed):
   wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
   bash Miniconda3-latest-Linux-x86_64.sh

2. Install Ollama (if not already installed):
   curl -fsSL https://ollama.ai/install.sh | sh

3. If using .gguf model, have your model file ready:
   - Example: /path/to/your/model.gguf

4. If using existing Ollama model, pull it first:
   ollama pull llama3.1:8b
   ollama pull mistral-nemo:latest

üéØ Running the Scanner:
=====================
python garak_prompt_injection.py

üìä Available Test Probes (32 total):
===================================
Core Vulnerability Probes:
‚Ä¢ promptinject - Primary prompt injection tests
‚Ä¢ dan - "Do Anything Now" jailbreak attempts
‚Ä¢ encoding - Encoding-based injection tests
‚Ä¢ latentinjection - Hidden injection tests

Advanced Security Probes:
‚Ä¢ exploitation - Code and template injection
‚Ä¢ malwaregen - Malware generation attempts
‚Ä¢ xss - Cross-site scripting tests
‚Ä¢ visual_jailbreak - Visual-based jailbreak attempts

Specialized Attack Probes:
‚Ä¢ continuation - Continuation attacks
‚Ä¢ suffix - Suffix-based attacks
‚Ä¢ grandma - Social engineering tests
‚Ä¢ doctor - Professional role exploitation
‚Ä¢ ansiescape - ANSI escape sequence attacks
‚Ä¢ taptest - TAP (Tree of Attack Prompts) testing
‚Ä¢ ...and 18 more specialized probes

üí° Pro Tips:
============
‚Ä¢ Start with 'promptinject' for basic vulnerability testing
‚Ä¢ Use 'dan' for jailbreak resistance testing
‚Ä¢ Use 'all' to run comprehensive security scanning
‚Ä¢ Tests can take several minutes to complete
‚Ä¢ Reports are automatically opened in your browser
‚Ä¢ Results are saved in current directory or ~/.garak/

üîç Example Test Scenarios:
==========================

Scenario 1: Testing a downloaded .gguf model
============================================
Step 1: Model Selection
- Choose option 1 (LLM model in .gguf format)
- Provide path to your .gguf file
- Give your model a name

Step 2: Environment Setup
- Choose option 1 to create environment
- Script creates conda environment and installs garak

Step 3: Probe Selection
- Select probes (recommend: promptinject, dan, encoding)

Step 4: Test Execution
- Wait for scanning to complete

Step 5: Report Review
- HTML report opens automatically in browser

Scenario 2: Testing an existing Ollama model
============================================
Step 1: Model Selection
- Choose option 2 (Already pulled model in Ollama)
- Select from available models

Step 2: Environment Setup
- Choose option 2 if environment already exists
- Provide environment name

Step 3: Probe Selection
- Select probes for testing

Step 4: Test Execution
- Monitor live scanning progress

Step 5: Report Review
- Review detailed vulnerability report

Scenario 3: Comprehensive Security Audit
=========================================
Step 1: Model Selection
- Use production-ready model

Step 2: Environment Setup
- Use dedicated security testing environment

Step 3: Probe Selection
- Select 'all' for comprehensive testing

Step 4: Test Execution
- Extended scanning time (30+ minutes)

Step 5: Report Review
- Detailed security audit report

üõ°Ô∏è Security Best Practices:
===========================
‚Ä¢ Run tests in isolated environments
‚Ä¢ Don't use production models for testing
‚Ä¢ Review results with security experts
‚Ä¢ Keep test data non-sensitive
‚Ä¢ Regular testing for model updates
‚Ä¢ Document vulnerability findings
‚Ä¢ Monitor for new attack vectors

üìà Understanding Results:
========================
‚Ä¢ Green ‚úì: Test passed (model is secure)
‚Ä¢ Red ‚úó: Test failed (potential vulnerability)
‚Ä¢ Yellow ‚ö†: Warning or attention needed
‚Ä¢ Reports show attack success rates
‚Ä¢ HTML reports provide detailed analysis
‚Ä¢ JSON reports available for automation

üîÑ Typical Workflow:
===================
1. Prerequisites Check ‚Üí Install missing components
2. Model Selection ‚Üí Choose .gguf or Ollama model
3. Environment Setup ‚Üí Create or use existing environment
4. Probe Selection ‚Üí Choose security tests
5. Test Execution ‚Üí Wait for completion
6. Report Analysis ‚Üí Review vulnerability findings
7. Security Remediation ‚Üí Address identified issues
8. Retest ‚Üí Verify fixes

üÜò Troubleshooting:
==================
‚Ä¢ "Conda not found": Add conda to PATH
‚Ä¢ "Ollama not found": Install Ollama service
‚Ä¢ "Model creation failed": Check file permissions
‚Ä¢ "Garak installation failed": Check internet connection
‚Ä¢ "Environment not found": Verify environment name
‚Ä¢ "No report generated": Check current directory

üöÄ Advanced Usage:
==================
‚Ä¢ Custom probe combinations for specific threat models
‚Ä¢ Automated testing with CI/CD pipelines
‚Ä¢ Batch testing multiple models
‚Ä¢ Integration with security monitoring systems
‚Ä¢ Custom report generation and analysis

Ready to scan your LLM for vulnerabilities? Run the script and follow the 5-step process!
""")

if __name__ == "__main__":
    print_usage_example()